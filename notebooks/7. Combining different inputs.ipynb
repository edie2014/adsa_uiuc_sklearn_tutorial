{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Heterogeneous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining multiple sources (and kinds) of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small variation of the newsgroups data, that is somewhat closer to what you will have after working for a while on feature engineering and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(\"datasets/tagged_newsgroup_subjects.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>target</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N , N N N</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Re : Goalie Mask Update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^ N ^ ,</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>Tigers pound Mariners !!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N , N</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>RE : survey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A U N , ^ N</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Dear Montana@pinetree.org Re : Hockey Pool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N , ! , A N , V A , ^</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Re : Goodbye , good riddance , get lost ' Stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pos              target  \\\n",
       "0              N , N N N    rec.sport.hockey   \n",
       "1                ^ N ^ ,  rec.sport.baseball   \n",
       "2                  N , N  rec.sport.baseball   \n",
       "3            A U N , ^ N    rec.sport.hockey   \n",
       "4  N , ! , A N , V A , ^    rec.sport.hockey   \n",
       "\n",
       "                                             token  \n",
       "0                          Re : Goalie Mask Update  \n",
       "1                    Tigers pound Mariners !!!!!!!  \n",
       "2                                      RE : survey  \n",
       "3       Dear Montana@pinetree.org Re : Hockey Pool  \n",
       "4  Re : Goodbye , good riddance , get lost ' Stars  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the \"token\" column is already tokenized (tokens are delimited by spaces) so there is no need to use a regular expression tokenizer.  We can just use `str.split`.\n",
    "\n",
    "The `pos` column contains the part-of-speech tag labels corresponding to the tokens. `N` means noun, `^` means proper noun, for example.\n",
    "\n",
    "This output was produced off-line using the CMU TweetNLP toolkit, which Vlad finds very robust to web text like this.\n",
    "\n",
    "But before we start, suppose we do even more feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>target</th>\n",
       "      <th>token</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N , N N N</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Re : Goalie Mask Update</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^ N ^ ,</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>Tigers pound Mariners !!!!!!!</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N , N</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>RE : survey</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A U N , ^ N</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Dear Montana@pinetree.org Re : Hockey Pool</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N , ! , A N , V A , ^</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Re : Goodbye , good riddance , get lost ' Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pos              target  \\\n",
       "0              N , N N N    rec.sport.hockey   \n",
       "1                ^ N ^ ,  rec.sport.baseball   \n",
       "2                  N , N  rec.sport.baseball   \n",
       "3            A U N , ^ N    rec.sport.hockey   \n",
       "4  N , ! , A N , V A , ^    rec.sport.hockey   \n",
       "\n",
       "                                             token is_reply  n_words  \n",
       "0                          Re : Goalie Mask Update     True        5  \n",
       "1                    Tigers pound Mariners !!!!!!!    False        4  \n",
       "2                                      RE : survey     True        3  \n",
       "3       Dear Montana@pinetree.org Re : Hockey Pool    False        6  \n",
       "4  Re : Goodbye , good riddance , get lost ' Stars     True       11  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"is_reply\"] = data.token.apply(lambda x: x.lower().startswith(\"re :\"))\n",
    "data[\"n_words\"] = data.token.apply(lambda x: len(x.split()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a pipe to extract the subject body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokenized(x):\n",
    "    return x[\"token\"]\n",
    "\n",
    "\n",
    "token_pipe = Pipeline([\n",
    "    ('proj', FunctionTransformer(\n",
    "                func=get_tokenized,\n",
    "                validate=False)),\n",
    "    ('vect', CountVectorizer(analyzer='word', tokenizer=str.split, min_df=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1197x319 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6345 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pipe.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '!!': 1,\n",
       " '!!!': 2,\n",
       " '!!!!': 3,\n",
       " '!!!!!': 4,\n",
       " '\"': 5,\n",
       " '&': 6,\n",
       " \"'\": 7,\n",
       " '(': 8,\n",
       " ')': 9,\n",
       " ',': 10,\n",
       " '-': 11,\n",
       " '.': 12,\n",
       " '...': 13,\n",
       " '....': 14,\n",
       " '1': 15,\n",
       " '18': 16,\n",
       " '19': 17,\n",
       " '1964': 18,\n",
       " '1988-1992': 19,\n",
       " '1992': 20,\n",
       " '1993': 21,\n",
       " '2': 22,\n",
       " '3': 23,\n",
       " '93': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '?': 27,\n",
       " '???': 28,\n",
       " '????': 29,\n",
       " 'a': 30,\n",
       " 'aargh': 31,\n",
       " 'abc': 32,\n",
       " 'again': 33,\n",
       " 'ahl': 34,\n",
       " 'al': 35,\n",
       " 'all': 36,\n",
       " 'all-time': 37,\n",
       " \"america's\": 38,\n",
       " 'an': 39,\n",
       " 'and': 40,\n",
       " 'another': 41,\n",
       " 'apr': 42,\n",
       " 'april': 43,\n",
       " 'are': 44,\n",
       " 'area': 45,\n",
       " 'astros': 46,\n",
       " 'at': 47,\n",
       " 'atlanta': 48,\n",
       " 'attendance': 49,\n",
       " 'averages': 50,\n",
       " 'back': 51,\n",
       " 'base': 52,\n",
       " 'baseball': 53,\n",
       " 'bay': 54,\n",
       " 'bbddd': 55,\n",
       " 'be': 56,\n",
       " 'beat': 57,\n",
       " 'best': 58,\n",
       " 'biggest': 59,\n",
       " 'bob': 60,\n",
       " 'bosox': 61,\n",
       " 'braves': 62,\n",
       " 'breaker': 63,\n",
       " 'bruins': 64,\n",
       " 'burns': 65,\n",
       " 'but': 66,\n",
       " 'canada': 67,\n",
       " 'canadian': 68,\n",
       " 'canadiens': 69,\n",
       " 'captains': 70,\n",
       " 'catchers': 71,\n",
       " 'cherry': 72,\n",
       " \"coach's\": 73,\n",
       " 'community': 74,\n",
       " 'conf': 75,\n",
       " 'corner': 76,\n",
       " 'coverage': 77,\n",
       " 'cubs': 78,\n",
       " 'cup': 79,\n",
       " 'dave': 80,\n",
       " 'day': 81,\n",
       " 'defensive': 82,\n",
       " 'deja': 83,\n",
       " 'detroit': 84,\n",
       " 'devils': 85,\n",
       " 'div': 86,\n",
       " 'don': 87,\n",
       " 'doubters': 88,\n",
       " 'draft': 89,\n",
       " 'dumb': 90,\n",
       " 'east': 91,\n",
       " 'espn': 92,\n",
       " 'europe': 93,\n",
       " 'europeans': 94,\n",
       " 'expansion': 95,\n",
       " 'expos': 96,\n",
       " 'fame': 97,\n",
       " 'fans': 98,\n",
       " 'final': 99,\n",
       " 'finals': 100,\n",
       " 'finland/sweden': 101,\n",
       " 'first': 102,\n",
       " 'flames': 103,\n",
       " 'flyers': 104,\n",
       " 'for': 105,\n",
       " 'from': 106,\n",
       " 'game': 107,\n",
       " 'games': 108,\n",
       " 'get': 109,\n",
       " 'giants': 110,\n",
       " 'gif': 111,\n",
       " 'gm': 112,\n",
       " 'go': 113,\n",
       " 'goalie': 114,\n",
       " 'golf': 115,\n",
       " 'good': 116,\n",
       " 'goodbye': 117,\n",
       " 'great': 118,\n",
       " 'habs': 119,\n",
       " 'hall': 120,\n",
       " 'have': 121,\n",
       " 'hawks': 122,\n",
       " 'hell': 123,\n",
       " 'help': 124,\n",
       " 'helsinki/stockholm': 125,\n",
       " 'henderson': 126,\n",
       " 'hispanic': 127,\n",
       " 'hmm': 128,\n",
       " 'hockey': 129,\n",
       " 'home': 130,\n",
       " 'homeruns': 131,\n",
       " 'how': 132,\n",
       " 'i': 133,\n",
       " 'idiot': 134,\n",
       " 'if': 135,\n",
       " 'in': 136,\n",
       " 'indians': 137,\n",
       " 'info': 138,\n",
       " 'is': 139,\n",
       " 'islanders': 140,\n",
       " 'isles': 141,\n",
       " 'it': 142,\n",
       " 'jack': 143,\n",
       " 'jays': 144,\n",
       " 'jewish': 145,\n",
       " 'jim': 146,\n",
       " 'joe': 147,\n",
       " 'keenan': 148,\n",
       " 'kingman': 149,\n",
       " 'la': 150,\n",
       " 'laugh': 151,\n",
       " 'leaders': 152,\n",
       " 'leafs': 153,\n",
       " 'lefebvre': 154,\n",
       " \"let's\": 155,\n",
       " 'list': 156,\n",
       " 'local': 157,\n",
       " 'lost': 158,\n",
       " 'mailing': 159,\n",
       " 'majors': 160,\n",
       " 'manager': 161,\n",
       " 'many': 162,\n",
       " 'marlins': 163,\n",
       " 'mask': 164,\n",
       " 'masks': 165,\n",
       " 'media': 166,\n",
       " 'mel': 167,\n",
       " 'mets': 168,\n",
       " 'minus': 169,\n",
       " 'mlb': 170,\n",
       " 'montreal/ottawa/phillie': 171,\n",
       " 'morris': 172,\n",
       " 'my': 173,\n",
       " 'names': 174,\n",
       " 'ncaa': 175,\n",
       " 'needed': 176,\n",
       " 'new': 177,\n",
       " 'next': 178,\n",
       " 'nhl': 179,\n",
       " 'nhlpa': 180,\n",
       " 'nl': 181,\n",
       " 'not': 182,\n",
       " 'notes': 183,\n",
       " 'now': 184,\n",
       " 'octopus': 185,\n",
       " 'of': 186,\n",
       " 'offense': 187,\n",
       " 'old': 188,\n",
       " 'on': 189,\n",
       " 'one': 190,\n",
       " 'opinion': 191,\n",
       " 'or': 192,\n",
       " 'out': 193,\n",
       " 'over': 194,\n",
       " 'parse': 195,\n",
       " 'part': 196,\n",
       " 'pat': 197,\n",
       " 'pens': 198,\n",
       " 'phillies': 199,\n",
       " 'phils': 200,\n",
       " 'picks': 201,\n",
       " 'pirates': 202,\n",
       " 'pitching': 203,\n",
       " 'pittsburgh': 204,\n",
       " 'played': 205,\n",
       " 'player': 206,\n",
       " 'players': 207,\n",
       " 'playoff': 208,\n",
       " 'playoffs': 209,\n",
       " 'pleasant': 210,\n",
       " 'please': 211,\n",
       " 'plus': 212,\n",
       " 'poll': 213,\n",
       " 'pool': 214,\n",
       " 'predictions': 215,\n",
       " 'president': 216,\n",
       " 'question': 217,\n",
       " 'questions': 218,\n",
       " 'quick': 219,\n",
       " 'radio': 220,\n",
       " 'rangers': 221,\n",
       " 'rbis': 222,\n",
       " 're': 223,\n",
       " 'real': 224,\n",
       " 'record': 225,\n",
       " 'red': 226,\n",
       " 'results': 227,\n",
       " 'returns': 228,\n",
       " 'review': 229,\n",
       " 'rickey': 230,\n",
       " 'riddance': 231,\n",
       " 'rockies': 232,\n",
       " 'roster': 233,\n",
       " 'royals': 234,\n",
       " 'rule': 235,\n",
       " 'rumor': 236,\n",
       " 'runs': 237,\n",
       " 'rushed': 238,\n",
       " 'sad': 239,\n",
       " 'sandberg': 240,\n",
       " 'schedule': 241,\n",
       " 'score': 242,\n",
       " 'scores': 243,\n",
       " 'season': 244,\n",
       " 'series': 245,\n",
       " 'sharks': 246,\n",
       " 'signs': 247,\n",
       " 'so': 248,\n",
       " 'sox': 249,\n",
       " 'spanky': 250,\n",
       " 'stadium': 251,\n",
       " 'standings': 252,\n",
       " 'stanley': 253,\n",
       " 'stars': 254,\n",
       " 'stat': 255,\n",
       " 'stations': 256,\n",
       " 'stats': 257,\n",
       " 'stop': 258,\n",
       " 'stuff': 259,\n",
       " 'summary': 260,\n",
       " 'surprises': 261,\n",
       " 'swing': 262,\n",
       " 'talk': 263,\n",
       " 'team': 264,\n",
       " 'teams': 265,\n",
       " 'tee': 266,\n",
       " 'tell': 267,\n",
       " 'the': 268,\n",
       " 'third': 269,\n",
       " 'this': 270,\n",
       " 'thumbs': 271,\n",
       " 'tickets': 272,\n",
       " 'tie': 273,\n",
       " 'tiebreaker': 274,\n",
       " 'tigers': 275,\n",
       " 'times': 276,\n",
       " 'to': 277,\n",
       " 'too': 278,\n",
       " 'toronto': 279,\n",
       " 'torre': 280,\n",
       " 'trade': 281,\n",
       " 'trivia': 282,\n",
       " 'truly': 283,\n",
       " 'tv': 284,\n",
       " 'two': 285,\n",
       " 'ulf': 286,\n",
       " 'uniforms': 287,\n",
       " 'up': 288,\n",
       " 'update': 289,\n",
       " 'updatedir': 290,\n",
       " 'usenet': 291,\n",
       " 'vs': 292,\n",
       " 'vu': 293,\n",
       " 'was': 294,\n",
       " 'way': 295,\n",
       " 'wc': 296,\n",
       " 'we': 297,\n",
       " 'week': 298,\n",
       " 'were': 299,\n",
       " 'wfan': 300,\n",
       " \"what's\": 301,\n",
       " 'when': 302,\n",
       " 'where': 303,\n",
       " 'who': 304,\n",
       " 'why': 305,\n",
       " 'will': 306,\n",
       " 'win': 307,\n",
       " 'wings': 308,\n",
       " 'wings-leafs': 309,\n",
       " 'winner': 310,\n",
       " 'with': 311,\n",
       " 'worst': 312,\n",
       " 'writer': 313,\n",
       " 'yankee': 314,\n",
       " \"year's\": 315,\n",
       " 'you': 316,\n",
       " 'young': 317,\n",
       " 'your': 318}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pipe.steps[1][1].vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a pipe to extract part-of-speech bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pos(x):\n",
    "    return x[\"pos\"]\n",
    "\n",
    "\n",
    "pos_pipe = Pipeline([\n",
    "    ('proj', FunctionTransformer(\n",
    "                func=get_pos,\n",
    "                validate=False)),\n",
    "    ('vect', CountVectorizer(analyzer='word',\n",
    "                             lowercase=False,\n",
    "                             tokenizer=str.split,\n",
    "                             ngram_range=(1, 2)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('proj', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function get_pos at 0x7f9dacb190d0>, pass_y=False,\n",
       "          validate=False)), ('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content...pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<method 'split' of 'str' objects>, vocabulary=None))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '! ,': 1,\n",
       " '! ^': 2,\n",
       " '$': 3,\n",
       " '$ &': 4,\n",
       " '$ ,': 5,\n",
       " '$ A': 6,\n",
       " '$ N': 7,\n",
       " '$ P': 8,\n",
       " '$ R': 9,\n",
       " '$ S': 10,\n",
       " '$ V': 11,\n",
       " '$ ^': 12,\n",
       " '&': 13,\n",
       " '& ,': 14,\n",
       " '& A': 15,\n",
       " '& D': 16,\n",
       " '& N': 17,\n",
       " '& R': 18,\n",
       " '& V': 19,\n",
       " '& Z': 20,\n",
       " '& ^': 21,\n",
       " ',': 22,\n",
       " ', !': 23,\n",
       " ', $': 24,\n",
       " ', &': 25,\n",
       " ', ,': 26,\n",
       " ', A': 27,\n",
       " ', D': 28,\n",
       " ', G': 29,\n",
       " ', L': 30,\n",
       " ', N': 31,\n",
       " ', O': 32,\n",
       " ', P': 33,\n",
       " ', R': 34,\n",
       " ', V': 35,\n",
       " ', Z': 36,\n",
       " ', ^': 37,\n",
       " 'A': 38,\n",
       " 'A $': 39,\n",
       " 'A &': 40,\n",
       " 'A ,': 41,\n",
       " 'A A': 42,\n",
       " 'A N': 43,\n",
       " 'A P': 44,\n",
       " 'A R': 45,\n",
       " 'A U': 46,\n",
       " 'A V': 47,\n",
       " 'A ^': 48,\n",
       " 'D': 49,\n",
       " 'D $': 50,\n",
       " 'D ,': 51,\n",
       " 'D A': 52,\n",
       " 'D D': 53,\n",
       " 'D N': 54,\n",
       " 'D S': 55,\n",
       " 'D ^': 56,\n",
       " 'E': 57,\n",
       " 'E E': 58,\n",
       " 'G': 59,\n",
       " 'G ,': 60,\n",
       " 'G G': 61,\n",
       " 'G N': 62,\n",
       " 'G ^': 63,\n",
       " 'L': 64,\n",
       " 'L P': 65,\n",
       " 'L R': 66,\n",
       " 'L V': 67,\n",
       " 'N': 68,\n",
       " 'N $': 69,\n",
       " 'N &': 70,\n",
       " 'N ,': 71,\n",
       " 'N A': 72,\n",
       " 'N E': 73,\n",
       " 'N G': 74,\n",
       " 'N N': 75,\n",
       " 'N O': 76,\n",
       " 'N P': 77,\n",
       " 'N R': 78,\n",
       " 'N V': 79,\n",
       " 'N ^': 80,\n",
       " 'O': 81,\n",
       " 'O ,': 82,\n",
       " 'O A': 83,\n",
       " 'O N': 84,\n",
       " 'O P': 85,\n",
       " 'O R': 86,\n",
       " 'O T': 87,\n",
       " 'O V': 88,\n",
       " 'P': 89,\n",
       " 'P $': 90,\n",
       " 'P ,': 91,\n",
       " 'P A': 92,\n",
       " 'P D': 93,\n",
       " 'P G': 94,\n",
       " 'P N': 95,\n",
       " 'P O': 96,\n",
       " 'P P': 97,\n",
       " 'P V': 98,\n",
       " 'P X': 99,\n",
       " 'P ^': 100,\n",
       " 'R': 101,\n",
       " 'R !': 102,\n",
       " 'R ,': 103,\n",
       " 'R A': 104,\n",
       " 'R D': 105,\n",
       " 'R N': 106,\n",
       " 'R P': 107,\n",
       " 'R R': 108,\n",
       " 'R V': 109,\n",
       " 'R ^': 110,\n",
       " 'S': 111,\n",
       " 'S A': 112,\n",
       " 'S N': 113,\n",
       " 'S V': 114,\n",
       " 'T': 115,\n",
       " 'T ,': 116,\n",
       " 'T D': 117,\n",
       " 'T P': 118,\n",
       " 'U': 119,\n",
       " 'U N': 120,\n",
       " 'V': 121,\n",
       " 'V $': 122,\n",
       " 'V ,': 123,\n",
       " 'V A': 124,\n",
       " 'V D': 125,\n",
       " 'V G': 126,\n",
       " 'V N': 127,\n",
       " 'V O': 128,\n",
       " 'V P': 129,\n",
       " 'V R': 130,\n",
       " 'V T': 131,\n",
       " 'V V': 132,\n",
       " 'V ^': 133,\n",
       " 'X': 134,\n",
       " 'X D': 135,\n",
       " 'Z': 136,\n",
       " 'Z $': 137,\n",
       " 'Z ,': 138,\n",
       " 'Z A': 139,\n",
       " 'Z N': 140,\n",
       " 'Z V': 141,\n",
       " 'Z Z': 142,\n",
       " '^': 143,\n",
       " '^ $': 144,\n",
       " '^ &': 145,\n",
       " '^ ,': 146,\n",
       " '^ A': 147,\n",
       " '^ D': 148,\n",
       " '^ E': 149,\n",
       " '^ G': 150,\n",
       " '^ N': 151,\n",
       " '^ O': 152,\n",
       " '^ P': 153,\n",
       " '^ R': 154,\n",
       " '^ T': 155,\n",
       " '^ V': 156,\n",
       " '^ X': 157,\n",
       " '^ Z': 158,\n",
       " '^ ^': 159}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pipe.steps[1][1].vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's extract the manually-computed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_manual(x):\n",
    "    res = x[[\"n_words\", \"is_reply\"]]  # select cols\n",
    "    return res.values.astype(np.double)  # convert to numpy\n",
    "\n",
    "\n",
    "manual = FunctionTransformer(func=extract_manual,\n",
    "                             validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  1.],\n",
       "       [ 4.,  0.],\n",
       "       [ 3.,  1.],\n",
       "       ..., \n",
       "       [ 4.,  1.],\n",
       "       [ 4.,  0.],\n",
       "       [ 5.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting them all together, side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "union = FeatureUnion([\n",
    "    (\"words\", token_pipe),\n",
    "    (\"pos\", pos_pipe),\n",
    "    (\"manual\", manual)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1197x481 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19198 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "481 is 319 (from tokens) + 160 (POS tag uni and bigrams) + 2.\n",
    "\n",
    "FeatureUnion concatenates them left-to-right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_pipe = Pipeline([\n",
    "    ('union', union),\n",
    "    ('scale', StandardScaler(with_mean=False)), \n",
    "        # Different features can end up on different scales.\n",
    "        # Some classifiers are not impacted much, but it can impact interpretation.\n",
    "        # Puzzle: can we use `with_mean=True?` Why not?\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint, expon, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(\n",
    "    full_pipe,\n",
    "    {\n",
    "        'union__words__vect__min_df': randint(1, 11),  # That's a mouthful!\n",
    "        'clf__C': expon(scale=10)\n",
    "    },\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=2,\n",
    "    n_iter=40,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('words', Pipeline(steps=[('proj', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function get_tokenized at 0x7f9dacb19158>, pass_y=False,\n",
       "          validate=False)), ('vect', CountVectorizer(analyzer='word', binary=False, de...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params={}, iid=True, n_iter=40, n_jobs=2,\n",
       "          param_distributions={'clf__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f9dac1bb2e8>, 'union__words__vect__min_df': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f9dacb24198>},\n",
       "          pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
       "          scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91395154553049285"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.87469, std: 0.01545, params: {'clf__C': 12.377127928425844, 'union__words__vect__min_df': 5},\n",
       " mean: 0.86466, std: 0.00938, params: {'clf__C': 7.440362212259898, 'union__words__vect__min_df': 9},\n",
       " mean: 0.86550, std: 0.00719, params: {'clf__C': 9.10738729525131, 'union__words__vect__min_df': 9},\n",
       " mean: 0.89307, std: 0.02050, params: {'clf__C': 10.574911418700976, 'union__words__vect__min_df': 3},\n",
       " mean: 0.87385, std: 0.02467, params: {'clf__C': 2.7707606872905277, 'union__words__vect__min_df': 6},\n",
       " mean: 0.85380, std: 0.01009, params: {'clf__C': 9.614176425640345, 'union__words__vect__min_df': 10},\n",
       " mean: 0.87302, std: 0.01127, params: {'clf__C': 23.15001589085309, 'union__words__vect__min_df': 6},\n",
       " mean: 0.90226, std: 0.02515, params: {'clf__C': 2.019699468304375, 'union__words__vect__min_df': 3},\n",
       " mean: 0.86383, std: 0.02008, params: {'clf__C': 0.5329835938608137, 'union__words__vect__min_df': 8},\n",
       " mean: 0.90977, std: 0.01748, params: {'clf__C': 7.54653097889178, 'union__words__vect__min_df': 2},\n",
       " mean: 0.85046, std: 0.00775, params: {'clf__C': 31.223249058808037, 'union__words__vect__min_df': 10},\n",
       " mean: 0.86048, std: 0.00719, params: {'clf__C': 20.21253796017777, 'union__words__vect__min_df': 9},\n",
       " mean: 0.84712, std: 0.00614, params: {'clf__C': 4.406232231628138, 'union__words__vect__min_df': 7},\n",
       " mean: 0.85046, std: 0.01199, params: {'clf__C': 1.7925455990437933, 'union__words__vect__min_df': 7},\n",
       " mean: 0.86884, std: 0.02090, params: {'clf__C': 18.988566329732596, 'union__words__vect__min_df': 4},\n",
       " mean: 0.85380, std: 0.01127, params: {'clf__C': 2.620006885554413, 'union__words__vect__min_df': 7},\n",
       " mean: 0.87302, std: 0.01127, params: {'clf__C': 20.439690401833058, 'union__words__vect__min_df': 6},\n",
       " mean: 0.87051, std: 0.02501, params: {'clf__C': 1.402372571700989, 'union__words__vect__min_df': 6},\n",
       " mean: 0.84795, std: 0.00515, params: {'clf__C': 4.163778955882861, 'union__words__vect__min_df': 10},\n",
       " mean: 0.90560, std: 0.02501, params: {'clf__C': 29.74672705375593, 'union__words__vect__min_df': 2},\n",
       " mean: 0.88889, std: 0.02475, params: {'clf__C': 35.1876034182429, 'union__words__vect__min_df': 3},\n",
       " mean: 0.83960, std: 0.01598, params: {'clf__C': 0.32946186442712333, 'union__words__vect__min_df': 10},\n",
       " mean: 0.87552, std: 0.01437, params: {'clf__C': 9.220181229539534, 'union__words__vect__min_df': 5},\n",
       " mean: 0.91145, std: 0.00625, params: {'clf__C': 1.4945595529222087, 'union__words__vect__min_df': 1},\n",
       " mean: 0.87970, std: 0.01952, params: {'clf__C': 1.7534233326184065, 'union__words__vect__min_df': 4},\n",
       " mean: 0.87636, std: 0.01452, params: {'clf__C': 7.287697220417142, 'union__words__vect__min_df': 5},\n",
       " mean: 0.86132, std: 0.01641, params: {'clf__C': 2.6706217258050415, 'union__words__vect__min_df': 8},\n",
       " mean: 0.87886, std: 0.01740, params: {'clf__C': 2.517656084015102, 'union__words__vect__min_df': 4},\n",
       " mean: 0.86967, std: 0.01952, params: {'clf__C': 4.517619265470002, 'union__words__vect__min_df': 6},\n",
       " mean: 0.91312, std: 0.00923, params: {'clf__C': 1.4267662427971866, 'union__words__vect__min_df': 2},\n",
       " mean: 0.91395, std: 0.00591, params: {'clf__C': 7.251570247740405, 'union__words__vect__min_df': 1},\n",
       " mean: 0.86967, std: 0.01432, params: {'clf__C': 6.22840161639353, 'union__words__vect__min_df': 6},\n",
       " mean: 0.84545, std: 0.00719, params: {'clf__C': 17.400492856991256, 'union__words__vect__min_df': 7},\n",
       " mean: 0.91312, std: 0.00473, params: {'clf__C': 4.561779116006452, 'union__words__vect__min_df': 1},\n",
       " mean: 0.88555, std: 0.02542, params: {'clf__C': 0.6695764520502521, 'union__words__vect__min_df': 5},\n",
       " mean: 0.87218, std: 0.02279, params: {'clf__C': 1.683009045321259, 'union__words__vect__min_df': 6},\n",
       " mean: 0.91228, std: 0.00541, params: {'clf__C': 3.8726285443280157, 'union__words__vect__min_df': 1},\n",
       " mean: 0.90810, std: 0.01740, params: {'clf__C': 10.472801712227778, 'union__words__vect__min_df': 2},\n",
       " mean: 0.87302, std: 0.01316, params: {'clf__C': 34.84984163783302, 'union__words__vect__min_df': 5},\n",
       " mean: 0.88053, std: 0.01857, params: {'clf__C': 0.1786623986939673, 'union__words__vect__min_df': 6}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
