{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Heterogeneous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining multiple sources (and kinds) of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small variation of the newsgroups data, that is somewhat closer to what you will have after working for a while on feature engineering and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(\"datasets/tagged_newsgroup_subjects.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>target</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N , N N N</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Re : Goalie Mask Update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^ N ^ ,</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>Tigers pound Mariners !!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N , N</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>RE : survey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A U N , ^ N</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Dear Montana@pinetree.org Re : Hockey Pool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N , ! , A N , V A , ^</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Re : Goodbye , good riddance , get lost ' Stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pos              target  \\\n",
       "0              N , N N N    rec.sport.hockey   \n",
       "1                ^ N ^ ,  rec.sport.baseball   \n",
       "2                  N , N  rec.sport.baseball   \n",
       "3            A U N , ^ N    rec.sport.hockey   \n",
       "4  N , ! , A N , V A , ^    rec.sport.hockey   \n",
       "\n",
       "                                             token  \n",
       "0                          Re : Goalie Mask Update  \n",
       "1                    Tigers pound Mariners !!!!!!!  \n",
       "2                                      RE : survey  \n",
       "3       Dear Montana@pinetree.org Re : Hockey Pool  \n",
       "4  Re : Goodbye , good riddance , get lost ' Stars  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the \"token\" column is already tokenized (tokens are delimited by spaces) so there is no need to use a regular expression tokenizer.  We can just use `str.split`.\n",
    "\n",
    "The `pos` column contains the part-of-speech tag labels corresponding to the tokens. `N` means noun, `^` means proper noun, for example.\n",
    "\n",
    "This output was produced off-line using the CMU TweetNLP toolkit, which Vlad finds very robust to web text like this.\n",
    "\n",
    "But before we start, suppose we do even more feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>target</th>\n",
       "      <th>token</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N , N N N</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Re : Goalie Mask Update</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^ N ^ ,</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>Tigers pound Mariners !!!!!!!</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N , N</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>RE : survey</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A U N , ^ N</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Dear Montana@pinetree.org Re : Hockey Pool</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N , ! , A N , V A , ^</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Re : Goodbye , good riddance , get lost ' Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pos              target  \\\n",
       "0              N , N N N    rec.sport.hockey   \n",
       "1                ^ N ^ ,  rec.sport.baseball   \n",
       "2                  N , N  rec.sport.baseball   \n",
       "3            A U N , ^ N    rec.sport.hockey   \n",
       "4  N , ! , A N , V A , ^    rec.sport.hockey   \n",
       "\n",
       "                                             token is_reply  n_words  \n",
       "0                          Re : Goalie Mask Update     True        5  \n",
       "1                    Tigers pound Mariners !!!!!!!    False        4  \n",
       "2                                      RE : survey     True        3  \n",
       "3       Dear Montana@pinetree.org Re : Hockey Pool    False        6  \n",
       "4  Re : Goodbye , good riddance , get lost ' Stars     True       11  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"is_reply\"] = data.token.apply(lambda x: x.lower().startswith(\"re :\"))\n",
    "data[\"n_words\"] = data.token.apply(lambda x: len(x.split()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a pipe to extract the subject body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokenized(x):\n",
    "    return x[\"token\"]\n",
    "\n",
    "\n",
    "token_pipe = Pipeline([\n",
    "    ('proj', FunctionTransformer(\n",
    "                func=get_tokenized,\n",
    "                validate=False)),\n",
    "    ('vect', CountVectorizer(analyzer='word',\n",
    "                             token_pattern=r'(?u)\\b\\S+\\b',\n",
    "                             #tokenizer=unicode.split,\n",
    "                             min_df=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1197x299 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 4593 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pipe.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'1': 0,\n",
       " u'18': 1,\n",
       " u'19': 2,\n",
       " u'1964': 3,\n",
       " u'1988-1992': 4,\n",
       " u'1992': 5,\n",
       " u'1993': 6,\n",
       " u'2': 7,\n",
       " u'3': 8,\n",
       " u'93': 9,\n",
       " u'a': 10,\n",
       " u'aargh': 11,\n",
       " u'abc': 12,\n",
       " u'again': 13,\n",
       " u'ahl': 14,\n",
       " u'al': 15,\n",
       " u'all': 16,\n",
       " u'all-time': 17,\n",
       " u\"america's\": 18,\n",
       " u'an': 19,\n",
       " u'and': 20,\n",
       " u'another': 21,\n",
       " u'apr': 22,\n",
       " u'april': 23,\n",
       " u'are': 24,\n",
       " u'area': 25,\n",
       " u'astros': 26,\n",
       " u'at': 27,\n",
       " u'atlanta': 28,\n",
       " u'attendance': 29,\n",
       " u'averages': 30,\n",
       " u'back': 31,\n",
       " u'base': 32,\n",
       " u'baseball': 33,\n",
       " u'bay': 34,\n",
       " u'bbddd': 35,\n",
       " u'be': 36,\n",
       " u'beat': 37,\n",
       " u'best': 38,\n",
       " u'biggest': 39,\n",
       " u'bob': 40,\n",
       " u'bosox': 41,\n",
       " u'braves': 42,\n",
       " u'breaker': 43,\n",
       " u'bruins': 44,\n",
       " u'burns': 45,\n",
       " u'but': 46,\n",
       " u'canada': 47,\n",
       " u'canadian': 48,\n",
       " u'canadiens': 49,\n",
       " u'captains': 50,\n",
       " u'catchers': 51,\n",
       " u'cherry': 52,\n",
       " u\"coach's\": 53,\n",
       " u'community': 54,\n",
       " u'conf': 55,\n",
       " u'corner': 56,\n",
       " u'coverage': 57,\n",
       " u'cubs': 58,\n",
       " u'cup': 59,\n",
       " u'dave': 60,\n",
       " u'day': 61,\n",
       " u'defensive': 62,\n",
       " u'deja': 63,\n",
       " u'detroit': 64,\n",
       " u'devils': 65,\n",
       " u'div': 66,\n",
       " u'don': 67,\n",
       " u'doubters': 68,\n",
       " u'draft': 69,\n",
       " u'dumb': 70,\n",
       " u'east': 71,\n",
       " u'espn': 72,\n",
       " u'europe': 73,\n",
       " u'europeans': 74,\n",
       " u'expansion': 75,\n",
       " u'expos': 76,\n",
       " u'fame': 77,\n",
       " u'fans': 78,\n",
       " u'final': 79,\n",
       " u'finals': 80,\n",
       " u'finland/sweden': 81,\n",
       " u'first': 82,\n",
       " u'flames': 83,\n",
       " u'flyers': 84,\n",
       " u'for': 85,\n",
       " u'from': 86,\n",
       " u'game': 87,\n",
       " u'games': 88,\n",
       " u'get': 89,\n",
       " u'giants': 90,\n",
       " u'gif': 91,\n",
       " u'gm': 92,\n",
       " u'go': 93,\n",
       " u'goalie': 94,\n",
       " u'golf': 95,\n",
       " u'good': 96,\n",
       " u'goodbye': 97,\n",
       " u'great': 98,\n",
       " u'habs': 99,\n",
       " u'hall': 100,\n",
       " u'have': 101,\n",
       " u'hawks': 102,\n",
       " u'hell': 103,\n",
       " u'help': 104,\n",
       " u'helsinki/stockholm': 105,\n",
       " u'henderson': 106,\n",
       " u'hispanic': 107,\n",
       " u'hmm': 108,\n",
       " u'hockey': 109,\n",
       " u'home': 110,\n",
       " u'homeruns': 111,\n",
       " u'how': 112,\n",
       " u'i': 113,\n",
       " u'idiot': 114,\n",
       " u'if': 115,\n",
       " u'in': 116,\n",
       " u'indians': 117,\n",
       " u'info': 118,\n",
       " u'is': 119,\n",
       " u'islanders': 120,\n",
       " u'isles': 121,\n",
       " u'it': 122,\n",
       " u'jack': 123,\n",
       " u'jays': 124,\n",
       " u'jewish': 125,\n",
       " u'jim': 126,\n",
       " u'joe': 127,\n",
       " u'keenan': 128,\n",
       " u'kingman': 129,\n",
       " u'la': 130,\n",
       " u'laugh': 131,\n",
       " u'leaders': 132,\n",
       " u'leafs': 133,\n",
       " u'lefebvre': 134,\n",
       " u\"let's\": 135,\n",
       " u'list': 136,\n",
       " u'local': 137,\n",
       " u'lost': 138,\n",
       " u'mailing': 139,\n",
       " u'majors': 140,\n",
       " u'manager': 141,\n",
       " u'many': 142,\n",
       " u'marlins': 143,\n",
       " u'mask': 144,\n",
       " u'masks': 145,\n",
       " u'media': 146,\n",
       " u'mel': 147,\n",
       " u'mets': 148,\n",
       " u'minus': 149,\n",
       " u'mlb': 150,\n",
       " u'montreal/ottawa/phillie': 151,\n",
       " u'morris': 152,\n",
       " u'my': 153,\n",
       " u'names': 154,\n",
       " u'ncaa': 155,\n",
       " u'needed': 156,\n",
       " u'new': 157,\n",
       " u'next': 158,\n",
       " u'nhl': 159,\n",
       " u'nhlpa': 160,\n",
       " u'nl': 161,\n",
       " u'not': 162,\n",
       " u'notes': 163,\n",
       " u'now': 164,\n",
       " u'octopus': 165,\n",
       " u'of': 166,\n",
       " u'offense': 167,\n",
       " u'old': 168,\n",
       " u'on': 169,\n",
       " u'one': 170,\n",
       " u'opinion': 171,\n",
       " u'or': 172,\n",
       " u'out': 173,\n",
       " u'over': 174,\n",
       " u'parse': 175,\n",
       " u'part': 176,\n",
       " u'pat': 177,\n",
       " u'pens': 178,\n",
       " u'phillies': 179,\n",
       " u'phils': 180,\n",
       " u'picks': 181,\n",
       " u'pirates': 182,\n",
       " u'pitching': 183,\n",
       " u'pittsburgh': 184,\n",
       " u'played': 185,\n",
       " u'player': 186,\n",
       " u'players': 187,\n",
       " u'playoff': 188,\n",
       " u'playoffs': 189,\n",
       " u'pleasant': 190,\n",
       " u'please': 191,\n",
       " u'plus': 192,\n",
       " u'poll': 193,\n",
       " u'pool': 194,\n",
       " u'predictions': 195,\n",
       " u'president': 196,\n",
       " u'question': 197,\n",
       " u'questions': 198,\n",
       " u'quick': 199,\n",
       " u'radio': 200,\n",
       " u'rangers': 201,\n",
       " u'rbis': 202,\n",
       " u're': 203,\n",
       " u'real': 204,\n",
       " u'record': 205,\n",
       " u'red': 206,\n",
       " u'results': 207,\n",
       " u'returns': 208,\n",
       " u'review': 209,\n",
       " u'rickey': 210,\n",
       " u'riddance': 211,\n",
       " u'rockies': 212,\n",
       " u'roster': 213,\n",
       " u'royals': 214,\n",
       " u'rule': 215,\n",
       " u'rumor': 216,\n",
       " u'runs': 217,\n",
       " u'rushed': 218,\n",
       " u'sad': 219,\n",
       " u'sandberg': 220,\n",
       " u'schedule': 221,\n",
       " u'score': 222,\n",
       " u'scores': 223,\n",
       " u'season': 224,\n",
       " u'series': 225,\n",
       " u'sharks': 226,\n",
       " u'signs': 227,\n",
       " u'so': 228,\n",
       " u'sox': 229,\n",
       " u'spanky': 230,\n",
       " u'stadium': 231,\n",
       " u'standings': 232,\n",
       " u'stanley': 233,\n",
       " u'stars': 234,\n",
       " u'stat': 235,\n",
       " u'stations': 236,\n",
       " u'stats': 237,\n",
       " u'stop': 238,\n",
       " u'stuff': 239,\n",
       " u'summary': 240,\n",
       " u'surprises': 241,\n",
       " u'swing': 242,\n",
       " u'talk': 243,\n",
       " u'team': 244,\n",
       " u'teams': 245,\n",
       " u'tee': 246,\n",
       " u'tell': 247,\n",
       " u'the': 248,\n",
       " u'third': 249,\n",
       " u'this': 250,\n",
       " u'thumbs': 251,\n",
       " u'tickets': 252,\n",
       " u'tie': 253,\n",
       " u'tiebreaker': 254,\n",
       " u'tigers': 255,\n",
       " u'times': 256,\n",
       " u'to': 257,\n",
       " u'too': 258,\n",
       " u'toronto': 259,\n",
       " u'torre': 260,\n",
       " u'trade': 261,\n",
       " u'trivia': 262,\n",
       " u'truly': 263,\n",
       " u'tv': 264,\n",
       " u'two': 265,\n",
       " u'ulf': 266,\n",
       " u'uniforms': 267,\n",
       " u'up': 268,\n",
       " u'update': 269,\n",
       " u'updatedir': 270,\n",
       " u'usenet': 271,\n",
       " u'vs': 272,\n",
       " u'vu': 273,\n",
       " u'was': 274,\n",
       " u'way': 275,\n",
       " u'wc': 276,\n",
       " u'we': 277,\n",
       " u'week': 278,\n",
       " u'were': 279,\n",
       " u'wfan': 280,\n",
       " u\"what's\": 281,\n",
       " u'when': 282,\n",
       " u'where': 283,\n",
       " u'who': 284,\n",
       " u'why': 285,\n",
       " u'will': 286,\n",
       " u'win': 287,\n",
       " u'wings': 288,\n",
       " u'wings-leafs': 289,\n",
       " u'winner': 290,\n",
       " u'with': 291,\n",
       " u'worst': 292,\n",
       " u'writer': 293,\n",
       " u'yankee': 294,\n",
       " u\"year's\": 295,\n",
       " u'you': 296,\n",
       " u'young': 297,\n",
       " u'your': 298}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pipe.steps[1][1].vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a pipe to extract part-of-speech bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pos(x):\n",
    "    return x[\"pos\"]\n",
    "\n",
    "\n",
    "pos_pipe = Pipeline([\n",
    "    ('proj', FunctionTransformer(\n",
    "                func=get_pos,\n",
    "                validate=False)),\n",
    "    ('vect', CountVectorizer(analyzer='word',\n",
    "                             lowercase=False,\n",
    "                             token_pattern=r'(?u)\\b\\S+\\b',\n",
    "                             ngram_range=(1, 2)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('proj', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function get_pos at 0x7fd9b0edb230>, pass_y=False,\n",
       "          validate=False)), ('vect', CountVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'conte...      strip_accents=None, token_pattern='(?u)\\\\b\\\\S+\\\\b', tokenizer=None,\n",
       "        vocabulary=None))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'A': 0,\n",
       " u'A A': 1,\n",
       " u'A D': 2,\n",
       " u'A N': 3,\n",
       " u'A P': 4,\n",
       " u'A R': 5,\n",
       " u'A U': 6,\n",
       " u'A V': 7,\n",
       " u'D': 8,\n",
       " u'D A': 9,\n",
       " u'D D': 10,\n",
       " u'D N': 11,\n",
       " u'D R': 12,\n",
       " u'D S': 13,\n",
       " u'D V': 14,\n",
       " u'E': 15,\n",
       " u'E E': 16,\n",
       " u'G': 17,\n",
       " u'G G': 18,\n",
       " u'G N': 19,\n",
       " u'L': 20,\n",
       " u'L P': 21,\n",
       " u'L R': 22,\n",
       " u'L V': 23,\n",
       " u'N': 24,\n",
       " u'N A': 25,\n",
       " u'N D': 26,\n",
       " u'N E': 27,\n",
       " u'N G': 28,\n",
       " u'N L': 29,\n",
       " u'N N': 30,\n",
       " u'N O': 31,\n",
       " u'N P': 32,\n",
       " u'N R': 33,\n",
       " u'N T': 34,\n",
       " u'N V': 35,\n",
       " u'N Z': 36,\n",
       " u'O': 37,\n",
       " u'O A': 38,\n",
       " u'O N': 39,\n",
       " u'O P': 40,\n",
       " u'O R': 41,\n",
       " u'O T': 42,\n",
       " u'O V': 43,\n",
       " u'P': 44,\n",
       " u'P A': 45,\n",
       " u'P D': 46,\n",
       " u'P E': 47,\n",
       " u'P G': 48,\n",
       " u'P N': 49,\n",
       " u'P O': 50,\n",
       " u'P P': 51,\n",
       " u'P R': 52,\n",
       " u'P V': 53,\n",
       " u'P X': 54,\n",
       " u'R': 55,\n",
       " u'R A': 56,\n",
       " u'R D': 57,\n",
       " u'R N': 58,\n",
       " u'R P': 59,\n",
       " u'R R': 60,\n",
       " u'R V': 61,\n",
       " u'S': 62,\n",
       " u'S A': 63,\n",
       " u'S N': 64,\n",
       " u'S V': 65,\n",
       " u'T': 66,\n",
       " u'T D': 67,\n",
       " u'T P': 68,\n",
       " u'T R': 69,\n",
       " u'T V': 70,\n",
       " u'U': 71,\n",
       " u'U N': 72,\n",
       " u'V': 73,\n",
       " u'V A': 74,\n",
       " u'V D': 75,\n",
       " u'V G': 76,\n",
       " u'V N': 77,\n",
       " u'V O': 78,\n",
       " u'V P': 79,\n",
       " u'V R': 80,\n",
       " u'V S': 81,\n",
       " u'V T': 82,\n",
       " u'V V': 83,\n",
       " u'V X': 84,\n",
       " u'X': 85,\n",
       " u'X D': 86,\n",
       " u'Z': 87,\n",
       " u'Z A': 88,\n",
       " u'Z N': 89,\n",
       " u'Z V': 90,\n",
       " u'Z Z': 91}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pipe.steps[1][1].vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's extract the manually-computed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_manual(x):\n",
    "    res = x[[\"n_words\", \"is_reply\"]]  # select cols\n",
    "    return res.values.astype(np.double)  # convert to numpy\n",
    "\n",
    "\n",
    "manual = FunctionTransformer(func=extract_manual,\n",
    "                             validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  1.],\n",
       "       [ 4.,  0.],\n",
       "       [ 3.,  1.],\n",
       "       ..., \n",
       "       [ 4.,  1.],\n",
       "       [ 4.,  0.],\n",
       "       [ 5.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting them all together, side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "union = FeatureUnion([\n",
    "    (\"words\", token_pipe),\n",
    "    (\"pos\", pos_pipe),\n",
    "    (\"manual\", manual)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1197x393 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 12040 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeatureUnion concatenates them left-to-right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_pipe = Pipeline([\n",
    "    ('union', union),\n",
    "    ('scale', StandardScaler(with_mean=False)), \n",
    "        # Different features can end up on different scales.\n",
    "        # Some classifiers are not impacted much, but it can impact interpretation.\n",
    "        # Puzzle: can we use `with_mean=True?` Why not?\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint, expon, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(\n",
    "    full_pipe,\n",
    "    {\n",
    "        'union__words__vect__min_df': randint(1, 11),  # That's a mouthful!\n",
    "        'clf__C': expon(scale=10)\n",
    "    },\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=2,\n",
    "    n_iter=40,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('words', Pipeline(steps=[('proj', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function get_tokenized at 0x7fd9b37fbf50>, pass_y=False,\n",
       "          validate=False)), ('vect', CountVectorizer(analyzer='word', binary=False, de...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params={}, iid=True, n_iter=40, n_jobs=2,\n",
       "          param_distributions={'union__words__vect__min_df': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fd9b0ed2c10>, 'clf__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fd9b0edebd0>},\n",
       "          pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
       "          scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91478696741854637"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.91312, std: 0.00515, params: {'clf__C': 2.944138242601366, 'union__words__vect__min_df': 1},\n",
       " mean: 0.85213, std: 0.02156, params: {'clf__C': 10.383496509863521, 'union__words__vect__min_df': 6},\n",
       " mean: 0.85464, std: 0.01842, params: {'clf__C': 5.283913558856205, 'union__words__vect__min_df': 6},\n",
       " mean: 0.86132, std: 0.01316, params: {'clf__C': 1.5733142152836868, 'union__words__vect__min_df': 6},\n",
       " mean: 0.85380, std: 0.01740, params: {'clf__C': 6.192893708175707, 'union__words__vect__min_df': 6},\n",
       " mean: 0.86550, std: 0.01654, params: {'clf__C': 0.8301803940456774, 'union__words__vect__min_df': 6},\n",
       " mean: 0.84628, std: 0.00313, params: {'clf__C': 22.691879959868714, 'union__words__vect__min_df': 9},\n",
       " mean: 0.88053, std: 0.01654, params: {'clf__C': 36.53869157600396, 'union__words__vect__min_df': 5},\n",
       " mean: 0.88805, std: 0.01030, params: {'clf__C': 0.9460239418670159, 'union__words__vect__min_df': 5},\n",
       " mean: 0.83459, std: 0.02077, params: {'clf__C': 2.0439487093114552, 'union__words__vect__min_df': 10},\n",
       " mean: 0.85547, std: 0.01846, params: {'clf__C': 4.998877735918219, 'union__words__vect__min_df': 6},\n",
       " mean: 0.85380, std: 0.02090, params: {'clf__C': 30.779382304539162, 'union__words__vect__min_df': 6},\n",
       " mean: 0.84795, std: 0.00852, params: {'clf__C': 3.7576298316798873, 'union__words__vect__min_df': 9},\n",
       " mean: 0.84378, std: 0.01233, params: {'clf__C': 6.592011668255047, 'union__words__vect__min_df': 8},\n",
       " mean: 0.83793, std: 0.01654, params: {'clf__C': 16.72477868730451, 'union__words__vect__min_df': 7},\n",
       " mean: 0.91395, std: 0.00426, params: {'clf__C': 54.66406353728133, 'union__words__vect__min_df': 1},\n",
       " mean: 0.90393, std: 0.01901, params: {'clf__C': 14.040218365582554, 'union__words__vect__min_df': 2},\n",
       " mean: 0.81788, std: 0.01654, params: {'clf__C': 0.21271052189590883, 'union__words__vect__min_df': 10},\n",
       " mean: 0.87970, std: 0.02127, params: {'clf__C': 12.867275838566009, 'union__words__vect__min_df': 4},\n",
       " mean: 0.88388, std: 0.00827, params: {'clf__C': 2.772111396255257, 'union__words__vect__min_df': 5},\n",
       " mean: 0.83542, std: 0.02188, params: {'clf__C': 0.7974944362574325, 'union__words__vect__min_df': 10},\n",
       " mean: 0.88471, std: 0.00892, params: {'clf__C': 2.4141399034291897, 'union__words__vect__min_df': 5},\n",
       " mean: 0.88053, std: 0.01536, params: {'clf__C': 26.620936733892215, 'union__words__vect__min_df': 5},\n",
       " mean: 0.88972, std: 0.00938, params: {'clf__C': 16.242747263839345, 'union__words__vect__min_df': 3},\n",
       " mean: 0.84712, std: 0.00409, params: {'clf__C': 43.419855224718525, 'union__words__vect__min_df': 9},\n",
       " mean: 0.83459, std: 0.02213, params: {'clf__C': 0.8909302290245815, 'union__words__vect__min_df': 10},\n",
       " mean: 0.88471, std: 0.00892, params: {'clf__C': 2.4459100972338526, 'union__words__vect__min_df': 5},\n",
       " mean: 0.82540, std: 0.01250, params: {'clf__C': 0.09341913519563452, 'union__words__vect__min_df': 10},\n",
       " mean: 0.91395, std: 0.00967, params: {'clf__C': 0.3034721232963165, 'union__words__vect__min_df': 2},\n",
       " mean: 0.90977, std: 0.01139, params: {'clf__C': 4.942906865328535, 'union__words__vect__min_df': 2},\n",
       " mean: 0.83292, std: 0.02149, params: {'clf__C': 18.301160121555235, 'union__words__vect__min_df': 10},\n",
       " mean: 0.84294, std: 0.01127, params: {'clf__C': 13.14750677462544, 'union__words__vect__min_df': 8},\n",
       " mean: 0.85380, std: 0.02188, params: {'clf__C': 17.534948746053985, 'union__words__vect__min_df': 6},\n",
       " mean: 0.88137, std: 0.01009, params: {'clf__C': 6.394811729574049, 'union__words__vect__min_df': 5},\n",
       " mean: 0.85380, std: 0.01740, params: {'clf__C': 6.386214869140165, 'union__words__vect__min_df': 6},\n",
       " mean: 0.89641, std: 0.01050, params: {'clf__C': 2.5284298239509564, 'union__words__vect__min_df': 3},\n",
       " mean: 0.85297, std: 0.02050, params: {'clf__C': 23.480452960654723, 'union__words__vect__min_df': 6},\n",
       " mean: 0.89474, std: 0.00938, params: {'clf__C': 7.9943653054652595, 'union__words__vect__min_df': 3},\n",
       " mean: 0.83626, std: 0.01437, params: {'clf__C': 10.722040153997288, 'union__words__vect__min_df': 7},\n",
       " mean: 0.84461, std: 0.01342, params: {'clf__C': 5.376514591020909, 'union__words__vect__min_df': 7}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.grid_scores_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
