{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction with Bag-of-Words\n",
    "In many tasks, like in the classical spam detection, your input data is text.\n",
    "Free text with variables length is very far from the fixed length numeric representation that we need to do machine learning with scikit-learn.\n",
    "However, there is an easy and effective way to go from text data to a numeric representation that we can use with our models, called bag-of-words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/bag_of_words.svg\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that each sample in your dataset is represented as one string, which could be just a sentence, an email or a whole news article or book. To represent the sample, we first split the string into a list of tokens, which correspond to words. A simple way to do this to just split by whitespace, and then lowercase the word.\n",
    "\n",
    "\n",
    "Then, we built a vocabulary of all tokens (lowercased words) that appear in our whole dataset. This is usually a very large vocabulary.\n",
    "Finally, looking at our single sample, we could how often each word in the vocabulary appears.\n",
    "We represent our string by a vector, where each entry is how often a given word in the vocabular appears in the string.\n",
    "\n",
    "As each sample will only contain very few of the words, most entries will be zero, leading to a very high-dimensional but sparse representation.\n",
    "\n",
    "The method is called bag-of-words as the order of the words is lost entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [\"Some say the world will end in fire,\",\n",
    "     \"Some say in ice.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end': 0,\n",
       " 'fire': 1,\n",
       " 'ice': 2,\n",
       " 'in': 3,\n",
       " 'say': 4,\n",
       " 'some': 5,\n",
       " 'the': 6,\n",
       " 'will': 7,\n",
       " 'world': 8}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_bag_of_words = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end', 'fire', 'ice', 'in', 'say', 'some', 'the', 'will', 'world']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['end', 'fire', 'in', 'say', 'some', 'the', 'will', 'world'], \n",
       "       dtype='<U5'), array(['ice', 'in', 'say', 'some'], \n",
       "       dtype='<U5')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(X_bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf weighting\n",
    "A useful transformation that is often applied to the bag-of-word encoding is the so-called term-frequency inverse-document-frequency (Tfidf) scaling, which is a non-linear transformation of the word counts.\n",
    "\n",
    "The tf-idf method rescales words that are common to have less weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39  0.39  0.    0.28  0.28  0.28  0.39  0.39  0.39]\n",
      " [ 0.    0.    0.63  0.45  0.45  0.45  0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(tfidf_vectorizer.transform(X).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams and N-Grams\n",
    "Entirely discarding word order is not always a good idea, as composite phrases often have specific meaning, and modifiers like \"not\" can invert the meaning of words.\n",
    "A simple way to include some word order are n-grams, which don't only look at a single token, but at all pairs of neighborhing tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at sequences of tokens of minimum length 2 and maximum length 2\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "bigram_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end in',\n",
       " 'in fire',\n",
       " 'in ice',\n",
       " 'say in',\n",
       " 'say the',\n",
       " 'some say',\n",
       " 'the world',\n",
       " 'will end',\n",
       " 'world will']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to include unigrams (sigle tokens) and bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "gram_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end',\n",
       " 'end in',\n",
       " 'fire',\n",
       " 'ice',\n",
       " 'in',\n",
       " 'in fire',\n",
       " 'in ice',\n",
       " 'say',\n",
       " 'say in',\n",
       " 'say the',\n",
       " 'some',\n",
       " 'some say',\n",
       " 'the',\n",
       " 'the world',\n",
       " 'will',\n",
       " 'will end',\n",
       " 'world',\n",
       " 'world will']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_vectorizer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character n-grams\n",
    "=================\n",
    "Sometimes it is also helpful to not look at words, but instead single character.\n",
    "That is particularly useful if you have very noisy data, want to identify the language, or we want to predict something about a single word.\n",
    "We can simply look at characters instead of words by setting ``analyzer=\"char\"``.\n",
    "Looking at single characters is usually not very informative, but looking at longer n-grams of characters can be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vectorizer = CountVectorizer(ngram_range=(2, 2), analyzer=\"char\")\n",
    "char_vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' e', ' f', ' i', ' s', ' t', ' w', 'ay', 'ce', 'd ', 'e ', 'e,', 'e.', 'en', 'fi', 'he', 'ic', 'il', 'in', 'ir', 'l ', 'ld', 'll', 'me', 'n ', 'nd', 'om', 'or', 're', 'rl', 'sa', 'so', 'th', 'wi', 'wo', 'y ']\n"
     ]
    }
   ],
   "source": [
    "print(char_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"figures/supervised_scikit_learn.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do it for real now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             categories=('rec.sport.hockey', 'rec.sport.baseball'),\n",
    "                             remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs, y = dataset['data'], dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if\n",
      "team! \n",
      "Yeah but Soderstrom's mask has always appeared to be a lot bigger than the  \n",
      "average helmet-and-cage variety.  It has a certain appeal on its own\n",
      "\n",
      "josh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rec.sport.baseball', 'rec.sport.hockey']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names  # 0 = baseball, 1 = hockey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave out a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_train, docs_val, y_train, y_val = train_test_split(docs, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = vect.fit_transform(docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.98\n",
      "Validation accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "X_val = vect.transform(docs_val)\n",
    "print(\"Validation accuracy: {:.2f}\".format(clf.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What cases are we getting wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_proba = clf.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.53e-05,   1.00e+00],\n",
       "       [  9.95e-01,   5.04e-03],\n",
       "       [  9.39e-01,   6.07e-02],\n",
       "       [  1.36e-03,   9.99e-01],\n",
       "       [  8.33e-01,   1.67e-01],\n",
       "       [  4.20e-04,   1.00e+00],\n",
       "       [  9.90e-06,   1.00e+00],\n",
       "       [  9.33e-01,   6.72e-02],\n",
       "       [  4.09e-01,   5.91e-01],\n",
       "       [  8.43e-04,   9.99e-01],\n",
       "       [  9.96e-01,   4.49e-03],\n",
       "       [  3.99e-01,   6.01e-01],\n",
       "       [  1.46e-04,   1.00e+00],\n",
       "       [  3.03e-01,   6.97e-01],\n",
       "       [  9.42e-01,   5.76e-02],\n",
       "       [  2.56e-03,   9.97e-01],\n",
       "       [  5.86e-01,   4.14e-01],\n",
       "       [  1.01e-01,   8.99e-01],\n",
       "       [  9.98e-01,   2.43e-03],\n",
       "       [  4.73e-01,   5.27e-01],\n",
       "       [  1.96e-01,   8.04e-01],\n",
       "       [  8.73e-01,   1.27e-01],\n",
       "       [  1.16e-05,   1.00e+00],\n",
       "       [  3.13e-01,   6.87e-01],\n",
       "       [  4.77e-02,   9.52e-01],\n",
       "       [  9.81e-04,   9.99e-01],\n",
       "       [  7.93e-01,   2.07e-01],\n",
       "       [  1.72e-10,   1.00e+00],\n",
       "       [  9.56e-01,   4.36e-02],\n",
       "       [  1.21e-02,   9.88e-01],\n",
       "       [  9.98e-01,   2.48e-03],\n",
       "       [  1.12e-01,   8.88e-01],\n",
       "       [  2.13e-03,   9.98e-01],\n",
       "       [  8.38e-01,   1.62e-01],\n",
       "       [  1.00e+00,   2.88e-08],\n",
       "       [  1.00e+00,   1.01e-04],\n",
       "       [  9.35e-01,   6.51e-02],\n",
       "       [  3.54e-01,   6.46e-01],\n",
       "       [  3.93e-01,   6.07e-01],\n",
       "       [  6.36e-01,   3.64e-01],\n",
       "       [  9.87e-01,   1.29e-02],\n",
       "       [  1.03e-05,   1.00e+00],\n",
       "       [  2.94e-01,   7.06e-01],\n",
       "       [  5.30e-01,   4.70e-01],\n",
       "       [  9.82e-01,   1.85e-02],\n",
       "       [  7.25e-02,   9.27e-01],\n",
       "       [  5.68e-06,   1.00e+00],\n",
       "       [  3.66e-01,   6.34e-01],\n",
       "       [  8.98e-01,   1.02e-01],\n",
       "       [  6.18e-02,   9.38e-01],\n",
       "       [  6.01e-01,   3.99e-01],\n",
       "       [  6.78e-01,   3.22e-01],\n",
       "       [  9.50e-01,   5.01e-02],\n",
       "       [  5.58e-01,   4.42e-01],\n",
       "       [  3.69e-01,   6.31e-01],\n",
       "       [  5.06e-01,   4.94e-01],\n",
       "       [  1.00e+00,   3.36e-04],\n",
       "       [  6.82e-01,   3.18e-01],\n",
       "       [  9.12e-01,   8.84e-02],\n",
       "       [  6.89e-11,   1.00e+00],\n",
       "       [  1.40e-01,   8.60e-01],\n",
       "       [  2.59e-03,   9.97e-01],\n",
       "       [  5.00e-04,   1.00e+00],\n",
       "       [  4.44e-03,   9.96e-01],\n",
       "       [  9.33e-01,   6.71e-02],\n",
       "       [  9.90e-01,   9.76e-03],\n",
       "       [  9.99e-01,   5.65e-04],\n",
       "       [  3.44e-06,   1.00e+00],\n",
       "       [  9.98e-01,   1.59e-03],\n",
       "       [  1.83e-02,   9.82e-01],\n",
       "       [  7.89e-01,   2.11e-01],\n",
       "       [  8.84e-04,   9.99e-01],\n",
       "       [  9.56e-01,   4.45e-02],\n",
       "       [  7.37e-01,   2.63e-01],\n",
       "       [  5.86e-01,   4.14e-01],\n",
       "       [  1.00e+00,   3.55e-08],\n",
       "       [  8.91e-01,   1.09e-01],\n",
       "       [  9.21e-01,   7.92e-02],\n",
       "       [  1.39e-01,   8.61e-01],\n",
       "       [  4.08e-01,   5.92e-01],\n",
       "       [  5.14e-01,   4.86e-01],\n",
       "       [  9.99e-01,   8.30e-04],\n",
       "       [  1.00e+00,   1.83e-04],\n",
       "       [  1.00e+00,   4.76e-06],\n",
       "       [  3.46e-01,   6.54e-01],\n",
       "       [  2.11e-01,   7.89e-01],\n",
       "       [  2.59e-01,   7.41e-01],\n",
       "       [  9.97e-01,   3.23e-03],\n",
       "       [  1.95e-01,   8.05e-01],\n",
       "       [  2.50e-01,   7.50e-01],\n",
       "       [  5.48e-01,   4.52e-01],\n",
       "       [  5.39e-02,   9.46e-01],\n",
       "       [  4.22e-15,   1.00e+00],\n",
       "       [  7.58e-06,   1.00e+00],\n",
       "       [  7.80e-03,   9.92e-01],\n",
       "       [  3.28e-01,   6.72e-01],\n",
       "       [  8.89e-01,   1.11e-01],\n",
       "       [  3.05e-02,   9.69e-01],\n",
       "       [  1.09e-09,   1.00e+00],\n",
       "       [  8.28e-01,   1.72e-01],\n",
       "       [  7.43e-01,   2.57e-01],\n",
       "       [  5.86e-01,   4.14e-01],\n",
       "       [  9.93e-01,   6.94e-03],\n",
       "       [  9.99e-01,   7.29e-04],\n",
       "       [  1.72e-01,   8.28e-01],\n",
       "       [  6.01e-01,   3.99e-01],\n",
       "       [  2.60e-01,   7.40e-01],\n",
       "       [  5.26e-01,   4.74e-01],\n",
       "       [  4.32e-02,   9.57e-01],\n",
       "       [  9.61e-01,   3.89e-02],\n",
       "       [  8.53e-01,   1.47e-01],\n",
       "       [  3.49e-09,   1.00e+00],\n",
       "       [  2.55e-01,   7.45e-01],\n",
       "       [  1.19e-02,   9.88e-01],\n",
       "       [  9.51e-01,   4.93e-02],\n",
       "       [  9.38e-01,   6.25e-02],\n",
       "       [  2.37e-04,   1.00e+00],\n",
       "       [  9.96e-01,   4.12e-03],\n",
       "       [  9.99e-01,   1.49e-03],\n",
       "       [  1.09e-14,   1.00e+00],\n",
       "       [  1.00e+00,   1.89e-06],\n",
       "       [  9.77e-01,   2.32e-02],\n",
       "       [  8.60e-01,   1.40e-01],\n",
       "       [  9.60e-03,   9.90e-01],\n",
       "       [  7.88e-01,   2.12e-01],\n",
       "       [  4.96e-01,   5.04e-01],\n",
       "       [  1.24e-01,   8.76e-01],\n",
       "       [  9.52e-01,   4.81e-02],\n",
       "       [  1.42e-01,   8.58e-01],\n",
       "       [  8.33e-04,   9.99e-01],\n",
       "       [  7.63e-01,   2.37e-01],\n",
       "       [  8.90e-03,   9.91e-01],\n",
       "       [  8.23e-01,   1.77e-01],\n",
       "       [  1.44e-03,   9.99e-01],\n",
       "       [  7.97e-01,   2.03e-01],\n",
       "       [  8.76e-02,   9.12e-01],\n",
       "       [  1.00e+00,   3.25e-06],\n",
       "       [  5.96e-04,   9.99e-01],\n",
       "       [  1.33e-05,   1.00e+00],\n",
       "       [  4.07e-01,   5.93e-01],\n",
       "       [  8.63e-01,   1.37e-01],\n",
       "       [  2.48e-01,   7.52e-01],\n",
       "       [  5.77e-01,   4.23e-01],\n",
       "       [  3.94e-01,   6.06e-01],\n",
       "       [  9.92e-01,   8.13e-03],\n",
       "       [  7.09e-01,   2.91e-01],\n",
       "       [  8.78e-01,   1.22e-01],\n",
       "       [  1.00e+00,   2.64e-09],\n",
       "       [  6.29e-01,   3.71e-01],\n",
       "       [  1.79e-02,   9.82e-01],\n",
       "       [  1.39e-03,   9.99e-01],\n",
       "       [  1.00e+00,   2.13e-06],\n",
       "       [  9.99e-01,   1.21e-03],\n",
       "       [  9.94e-01,   5.93e-03],\n",
       "       [  3.37e-01,   6.63e-01],\n",
       "       [  4.27e-02,   9.57e-01],\n",
       "       [  9.94e-01,   6.03e-03],\n",
       "       [  1.00e+00,   1.50e-08],\n",
       "       [  9.65e-01,   3.49e-02],\n",
       "       [  7.57e-01,   2.43e-01],\n",
       "       [  9.98e-01,   1.54e-03],\n",
       "       [  4.91e-01,   5.09e-01],\n",
       "       [  9.78e-01,   2.24e-02],\n",
       "       [  8.20e-01,   1.80e-01],\n",
       "       [  4.53e-03,   9.95e-01],\n",
       "       [  9.74e-01,   2.62e-02],\n",
       "       [  8.36e-01,   1.64e-01],\n",
       "       [  7.81e-01,   2.19e-01],\n",
       "       [  1.81e-06,   1.00e+00],\n",
       "       [  2.17e-01,   7.83e-01],\n",
       "       [  2.93e-04,   1.00e+00],\n",
       "       [  1.69e-01,   8.31e-01],\n",
       "       [  6.53e-01,   3.47e-01],\n",
       "       [  4.54e-01,   5.46e-01],\n",
       "       [  9.92e-01,   7.59e-03],\n",
       "       [  5.59e-01,   4.41e-01],\n",
       "       [  9.34e-01,   6.56e-02],\n",
       "       [  5.02e-04,   9.99e-01],\n",
       "       [  7.03e-01,   2.97e-01],\n",
       "       [  1.12e-02,   9.89e-01],\n",
       "       [  6.79e-01,   3.21e-01],\n",
       "       [  3.00e-01,   7.00e-01],\n",
       "       [  9.00e-01,   9.97e-02],\n",
       "       [  9.69e-01,   3.09e-02],\n",
       "       [  1.55e-01,   8.45e-01],\n",
       "       [  5.13e-04,   9.99e-01],\n",
       "       [  5.86e-01,   4.14e-01],\n",
       "       [  5.53e-01,   4.47e-01],\n",
       "       [  7.28e-01,   2.72e-01],\n",
       "       [  9.96e-01,   3.82e-03],\n",
       "       [  2.49e-01,   7.51e-01],\n",
       "       [  1.21e-02,   9.88e-01],\n",
       "       [  9.98e-01,   1.74e-03],\n",
       "       [  1.09e-02,   9.89e-01],\n",
       "       [  7.45e-10,   1.00e+00],\n",
       "       [  1.92e-01,   8.08e-01],\n",
       "       [  9.02e-01,   9.78e-02],\n",
       "       [  1.14e-05,   1.00e+00],\n",
       "       [  9.91e-01,   9.03e-03],\n",
       "       [  3.58e-01,   6.42e-01],\n",
       "       [  9.93e-01,   6.75e-03],\n",
       "       [  8.89e-01,   1.11e-01],\n",
       "       [  9.99e-01,   5.69e-04],\n",
       "       [  3.70e-03,   9.96e-01],\n",
       "       [  5.45e-01,   4.55e-01],\n",
       "       [  1.37e-03,   9.99e-01],\n",
       "       [  5.70e-01,   4.30e-01],\n",
       "       [  6.05e-01,   3.95e-01],\n",
       "       [  4.19e-01,   5.81e-01],\n",
       "       [  3.38e-02,   9.66e-01],\n",
       "       [  2.92e-01,   7.08e-01],\n",
       "       [  7.00e-03,   9.93e-01],\n",
       "       [  1.90e-01,   8.10e-01],\n",
       "       [  9.90e-01,   1.02e-02],\n",
       "       [  8.47e-03,   9.92e-01],\n",
       "       [  7.61e-01,   2.39e-01],\n",
       "       [  9.06e-01,   9.41e-02],\n",
       "       [  7.06e-01,   2.94e-01],\n",
       "       [  7.82e-01,   2.18e-01],\n",
       "       [  9.80e-01,   2.01e-02],\n",
       "       [  1.53e-01,   8.47e-01],\n",
       "       [  1.64e-01,   8.36e-01],\n",
       "       [  8.16e-01,   1.84e-01],\n",
       "       [  1.00e+00,   1.24e-05],\n",
       "       [  9.28e-01,   7.24e-02],\n",
       "       [  1.00e+00,   2.37e-07],\n",
       "       [  9.45e-01,   5.45e-02],\n",
       "       [  8.66e-01,   1.34e-01],\n",
       "       [  8.69e-01,   1.31e-01],\n",
       "       [  0.00e+00,   1.00e+00],\n",
       "       [  9.46e-01,   5.42e-02],\n",
       "       [  9.67e-01,   3.29e-02],\n",
       "       [  7.40e-01,   2.60e-01],\n",
       "       [  9.03e-01,   9.70e-02],\n",
       "       [  9.83e-05,   1.00e+00],\n",
       "       [  1.00e+00,   2.59e-11],\n",
       "       [  2.04e-07,   1.00e+00],\n",
       "       [  9.99e-01,   1.50e-03],\n",
       "       [  9.06e-01,   9.35e-02],\n",
       "       [  8.54e-01,   1.46e-01],\n",
       "       [  9.81e-01,   1.91e-02],\n",
       "       [  6.06e-01,   3.94e-01],\n",
       "       [  9.96e-01,   4.40e-03],\n",
       "       [  1.00e+00,   2.83e-06],\n",
       "       [  2.01e-05,   1.00e+00],\n",
       "       [  4.86e-01,   5.14e-01],\n",
       "       [  5.74e-01,   4.26e-01],\n",
       "       [  2.76e-02,   9.72e-01],\n",
       "       [  3.67e-01,   6.33e-01],\n",
       "       [  5.86e-01,   4.14e-01],\n",
       "       [  9.99e-01,   9.41e-04],\n",
       "       [  4.84e-01,   5.16e-01],\n",
       "       [  9.14e-01,   8.63e-02],\n",
       "       [  6.80e-05,   1.00e+00],\n",
       "       [  1.36e-02,   9.86e-01],\n",
       "       [  3.08e-02,   9.69e-01],\n",
       "       [  5.82e-03,   9.94e-01],\n",
       "       [  7.72e-01,   2.28e-01],\n",
       "       [  1.20e-04,   1.00e+00],\n",
       "       [  1.00e+00,   5.13e-05],\n",
       "       [  6.39e-01,   3.61e-01],\n",
       "       [  7.87e-02,   9.21e-01],\n",
       "       [  8.13e-01,   1.87e-01],\n",
       "       [  5.86e-01,   4.14e-01],\n",
       "       [  9.77e-01,   2.33e-02],\n",
       "       [  9.99e-01,   8.94e-04],\n",
       "       [  5.76e-01,   4.24e-01],\n",
       "       [  6.11e-02,   9.39e-01],\n",
       "       [  9.94e-01,   6.35e-03],\n",
       "       [  4.88e-01,   5.12e-01],\n",
       "       [  6.30e-01,   3.70e-01],\n",
       "       [  1.00e+00,   8.09e-14],\n",
       "       [  3.31e-01,   6.69e-01],\n",
       "       [  1.44e-12,   1.00e+00],\n",
       "       [  2.95e-01,   7.05e-01],\n",
       "       [  3.31e-02,   9.67e-01],\n",
       "       [  1.27e-01,   8.73e-01],\n",
       "       [  3.77e-02,   9.62e-01],\n",
       "       [  9.46e-05,   1.00e+00],\n",
       "       [  1.00e+00,   4.98e-04],\n",
       "       [  1.70e-01,   8.30e-01],\n",
       "       [  9.62e-01,   3.76e-02],\n",
       "       [  5.62e-01,   4.38e-01],\n",
       "       [  4.94e-01,   5.06e-01],\n",
       "       [  3.58e-07,   1.00e+00],\n",
       "       [  5.84e-01,   4.16e-01],\n",
       "       [  2.02e-01,   7.98e-01],\n",
       "       [  9.05e-01,   9.50e-02],\n",
       "       [  4.61e-01,   5.39e-01],\n",
       "       [  4.06e-02,   9.59e-01],\n",
       "       [  8.10e-01,   1.90e-01],\n",
       "       [  5.92e-01,   4.08e-01],\n",
       "       [  9.99e-01,   1.41e-03],\n",
       "       [  4.64e-01,   5.36e-01],\n",
       "       [  1.00e+00,   6.64e-07],\n",
       "       [  5.51e-01,   4.49e-01],\n",
       "       [  8.22e-01,   1.78e-01],\n",
       "       [  6.55e-01,   3.45e-01],\n",
       "       [  0.00e+00,   1.00e+00],\n",
       "       [  9.84e-01,   1.55e-02]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0791647833328\n",
      "\n",
      "In case anyone missed it, I'm reposting this and I'm also selling some other\n",
      "stuff.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I've sold one, but I still have 2 left for sale. I also realize that $45 is\n",
      "alot of money, especially if you don't normally collect cards. So if enough\n",
      "people are interested, I'll break up the set into team sets. I'm not sure\n",
      "how much for each. It would be nice to just sell them for $3 each, but then\n",
      "the people who get the Whalers and Devils (Note, I'm not bagging on these teams\n",
      "its just that they don't have alot of good rookie cards in this set) would\n",
      "be subsidizing the people who want Chicago or Pittsburgh. So I'll have to make\n",
      "it varialble pricing. But most of them should be about $2 or $3 dollars.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ok someone asked for this one, but he's from Canada, if he can get me the\n",
      "be the alternate.\n",
      "\n",
      "Also I would like to sell 2 Upperdeck Pavel Bure rookie cards (note these\n",
      "are not in the UD low #'s set mentioned above). $16 each. They are $15 in\n",
      "the book, but the $1 goes for postage, packaging and insurance.\n",
      "\n",
      "And if there is something you want that you don't see, e-mail me, I may have\n",
      "it or may be able to get  it for you.\n",
      "\n",
      "0.0978019427122\n",
      "In  <1qvos8$r78@cl.msu.>, vergolin@euler.lbs.msu.edu (David Vergolini) writes...\n",
      "\n",
      "There's quite a few Wings fans lurking about here, they just tend\n",
      "to be low key and thoughtful rather than woofers.  I suppose every\n",
      "family must have a Roger Clinton, though.  But remember (to paraphrase\n",
      "one of my favorite Star Trek lines), \"if we adopt the ways of the Leaf\n",
      "fans, we are as bad as the Leaf fans\".\n",
      "\n",
      "Ron\n",
      "\n",
      "0.0997438600837\n",
      "\n",
      "I think the three-headed GM's guiding principle was to keep veterans\n",
      "in favor of youngsters only if they offered a \"significant\" advantage.\n",
      "At the end of last season, the contracts of several veterans with somewhat\n",
      "maginal contributions (Fenton, Bozek, Anderson, and a couple others I\n",
      "can't remember) were bought out. The idea was that youngsters could\n",
      "play almost as well, and had the potential to improve where these\n",
      "older guys did not. \n",
      "And they traded Mullen, because he wanted to go, not because he\n",
      "wasn't good enough, but I think they were a bit too optimistic\n",
      "in thinking they could make up for his contributions.\n",
      "An example from this season, Skriko was brought in on a trial basis\n",
      "but not kept, because of his age. I thought he was a decent\n",
      "contributor worth keeping around.\n",
      "\n",
      "The youth movement has its advantages; look at Gaudreau who\n",
      "might still be in KC if more veterans had been kept around. But\n",
      "you have to find the right balance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confidence_in_hockey = y_proba[y_val == 1, 1]\n",
    "confidence_in_baseball = y_proba[y_val == 0, 0]\n",
    "\n",
    "docs_about_hockey = np.array(docs_val)[y_val == 1]\n",
    "for ix in np.argsort(confidence_in_hockey)[:3]:\n",
    "    print(confidence_in_hockey[ix])\n",
    "    print(docs_about_hockey[ix])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.112150564452\n",
      "\n",
      "\n",
      "What makes you think Buck will still be in New York at year's end with\n",
      "George back?  :-)\n",
      "\n",
      "--\n",
      "    Keith Keller\t\t\t\tLET'S GO RANGERS!!!!!\n",
      "\t\t\t\t\t\tLET'S GO QUAKERS!!!!!\n",
      "\tkkeller@mail.sas.upenn.edu\t\tIVY LEAGUE CHAMPS!!!!\n",
      "\n",
      "0.139498219786\n",
      "Here's an easy question for someone who knows nothing about baseball...\n",
      "\n",
      "   What city do the California Angels play out of?\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "Richard J. Rauser        \"You have no idea what you're doing.\"\n",
      "rauser@sfu.ca            \"Oh, don't worry about that. We're professional\n",
      "WNI                          outlaws - we do this for a living.\"\n",
      "\n",
      "0.168970202287\n",
      "Hello, I'm doing a paper on censorship in music and I would appreciate it if you took the time to participate in this survey.  Please answer as each question asks ('why?' simply means that you have room to explain your answer, if you chose.).  The last question is for any comments, questions, or suggestions.  Thank you in advance, please E-mail to the address at the end.\n",
      "\n",
      "I)  are you [male/female]\n",
      "II) what is your age? \n",
      "III)what is your major/occupation?\n",
      "IV) what type of music do you listen to (check all that apply)?\n",
      "      a.  hard rock   b.  metal   c.  alternative   d.  blues    e.  rap\n",
      "      f.  jazz    g.  soft rock   h.  easy listening   i.  country   \n",
      "      j.  classical   k.  hard core   l.  dance   m.  new age\n",
      "      n.  others (did I miss any?)____________\n",
      "\n",
      "1)  Do you think recordings with objectionable or offensive lyrics be labeled? [yes/no] Why?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2)  Do you think certain recordings should be banned from minors (under 18 years of age)? [yes/no] why?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3)  Do you think certain recordings should be banned.  Period.  [yes/no]  Why?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4)  If yes to any of the above, who should decide:\n",
      "       a. parents\n",
      "       b. government\n",
      "       c. music industry\n",
      "       d. other________________\n",
      "\n",
      "feel free to add any comments on this.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5)  Do you think [more/less] should be done for controling record sales, or do you think the present labeling system is enough?  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6)  What is your definition of censorship?  Also, feel free to add comments, suggestions, questions, or further explanations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please E-mail at: mtt@kepler.unh.edu or hit 'R' to reply.\n",
      "\n",
      "thanks.\n",
      "Matthew T. Thompson\n",
      "\n",
      "\n",
      "disclaimer:  if any responses are used in paper, they will be anoynamous (sp?) unless the person specifies they what their name to be used.\n",
      "\n",
      "\n",
      "-- \n",
      "*************This .sig is closed for repairs********************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs_about_baseball = np.array(docs_val)[y_val == 0]\n",
    "for ix in np.argsort(confidence_in_baseball)[:3]:\n",
    "    print(confidence_in_baseball[ix])\n",
    "    print(docs_about_baseball[ix])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
